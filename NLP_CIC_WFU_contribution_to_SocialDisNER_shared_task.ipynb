{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajtamayoh/NLP-CIC-WFU-Contribution-to-SocialDisNER-shared-task-2022/blob/main/NLP_CIC_WFU_contribution_to_SocialDisNER_shared_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP-CIC-WFU contribution to SocialDisNER 2022\n",
        "\n",
        "Here you are the source code for the paper:\n",
        "\n",
        "### NLP-CIC-WFU at SocialDisNER: Disease Mention Extraction in Spanish Tweets Using Transfer Learning and Search by Propagation\n",
        "\n",
        "Authors:\n",
        "\n",
        "Antonio Tamayo (ajtamayo2019@ipn.cic.mx, ajtamayoh@gmail.com)\n",
        "\n",
        "Diego A. Burgos (burgosda@wfu.edu)\n",
        "\n",
        "Alexander Gelbulkh (gelbukh@gelbukh.com)\n",
        "\n",
        "For bugs or questions related to the code, do not hesitate to contact us (Antonio Tamayo: ajtamayoh@gmail.com)\n",
        "\n",
        "If you use this code please cite our work:\n",
        "\n",
        "Tamayo, A., Gelbukh, A., & Burgos, D. A. (2022, October). Nlp-cic-wfu at socialdisner: Disease mention extraction in spanish tweets using transfer learning and search by propagation. In Proceedings of The Seventh Workshop on Social Media Mining for Health Applications, Workshop & Shared Task (pp. 19-22).\n",
        "\n"
      ],
      "metadata": {
        "id": "Nml9hXt1NHXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements\n",
        "\n",
        "To run this code you need to download the dataset at: https://drive.google.com/drive/folders/1q6eZwL7sNTupRQW_bNkISvxY9t-zBsdu?usp=sharing\n",
        "\n",
        "Then, you must create a folder called \"Dataset\" in the root of your Google Drive and load there both folders and the file called mentions.tsv previously downloaded.\n",
        "\n",
        "Once the dataset is ready to use, you should [open this notebook in colab](https://colab.research.google.com/github/ajtamayoh/NLP-CIC-WFU-Contribution-to-SocialDisNER-shared-task-2022/blob/main/Code.ipynb) and save a copy in your drive."
      ],
      "metadata": {
        "id": "6S9L_KErP3yM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About the infrastructure"
      ],
      "metadata": {
        "id": "3gGu8XvkQuHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "9YCD8Yn8QvCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "PDijHzOMQzwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Google drive"
      ],
      "metadata": {
        "id": "U64pk_flQ-nw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgzIfAnfaDR1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1ncKARiLoRQ"
      },
      "source": [
        "## Exploring & Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZaPi7EMLrz8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PLlHg0aLtGp"
      },
      "outputs": [],
      "source": [
        "socialdisner_training = pd.read_csv(\"/content/drive/MyDrive/Dataset/mentions.tsv\", delimiter=\"\\t\")\n",
        "socialdisner_training.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnhX_oQ5CJfh"
      },
      "outputs": [],
      "source": [
        "text_files_path = \"/content/drive/MyDrive/Dataset/train-valid-txt-files/training/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNIun4SYCzP5"
      },
      "outputs": [],
      "source": [
        "f = open(text_files_path + str(socialdisner_training.iloc[1,0]) + \".txt\", \"r\", encoding=\"UTF-8\")\n",
        "for l in f:\n",
        "  print(l)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "socialdisner_training[\"tweets_id\"].unique().shape"
      ],
      "metadata": {
        "id": "W_PljfGy0hSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhs3W80KM7ZG"
      },
      "outputs": [],
      "source": [
        "#Tweets\n",
        "Tws = {}\n",
        "Tws_ids_training = []\n",
        "for fname in socialdisner_training[\"tweets_id\"].unique():\n",
        "  try:\n",
        "    with open(text_files_path + str(fname) + \".txt\", \"r\", encoding=\"UTF-8\") as f:\n",
        "      Tws.update({fname: f.read()})\n",
        "      Tws_ids_training.append(fname)\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPDGRBTdLw_W"
      },
      "outputs": [],
      "source": [
        "len(Tws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D_CPkKjsCzz"
      },
      "outputs": [],
      "source": [
        "#Diseases\n",
        "ENF = {}\n",
        "enfermedades = []\n",
        "fn = socialdisner_training[\"tweets_id\"][0]\n",
        "for fname, enf in zip(socialdisner_training[\"tweets_id\"], socialdisner_training[\"extraction\"]):\n",
        "    try:\n",
        "      if Tws[fname]: #To take only the diseases in the training dataset\n",
        "        if fname!=fn:\n",
        "          enfermedades = []\n",
        "        enfermedades.append(enf)\n",
        "        ENF.update({fname: enfermedades})\n",
        "        fn = fname\n",
        "    except:\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-aDUgrHULAi"
      },
      "outputs": [],
      "source": [
        "len(ENF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNRlMovPn5mi"
      },
      "outputs": [],
      "source": [
        "Tws[1494382407398662147]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjmYjm_u7LBl"
      },
      "outputs": [],
      "source": [
        "ENF[1494382407398662147]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization using SpaCy"
      ],
      "metadata": {
        "id": "4ZSJ6gatRc9r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocmTmRkiLtJM"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.es import Spanish\n",
        "nlp = Spanish()\n",
        "# Create a Tokenizer with the default settings for Spanish\n",
        "# including punctuation rules and exceptions\n",
        "tokenizer_spacy = nlp.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orQHlLHJM7b3"
      },
      "outputs": [],
      "source": [
        "Tws_tokenized = []\n",
        "for tw in Tws:\n",
        "    tx = []\n",
        "    tokens = tokenizer_spacy(Tws[tw])\n",
        "    #tokens = HCs[hc].split(\" \") #The simplest option. It was not used in our work.\n",
        "    for t in tokens:\n",
        "        tx.append(str(t))\n",
        "    Tws_tokenized.append(tx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rClFCgxnM7e4"
      },
      "outputs": [],
      "source": [
        "len(Tws_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0v803fyUFSM"
      },
      "outputs": [],
      "source": [
        "Tws_tokenized[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xE2NudYNHey"
      },
      "outputs": [],
      "source": [
        "Ent_tokenized = []\n",
        "for enf in ENF:\n",
        "    Tks = []\n",
        "    for e in ENF[enf]:\n",
        "      sl = []\n",
        "      tokens = tokenizer_spacy(e)\n",
        "      #tokens = e.split(\" \")\n",
        "      for t in tokens:\n",
        "          sl.append(str(t))\n",
        "      Tks.append(sl)\n",
        "    Ent_tokenized.append(Tks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMuPfY-XNHh4"
      },
      "outputs": [],
      "source": [
        "len(Ent_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR--IobbNHj6"
      },
      "outputs": [],
      "source": [
        "Ent_tokenized[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQu4uagdNePZ"
      },
      "source": [
        "## Tagging Data with BIO scheme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKy-FSGdiWhY"
      },
      "outputs": [],
      "source": [
        "def find_idx(list_to_check, item_to_find):\n",
        "    indices = []\n",
        "    for idx, value in enumerate(list_to_check):\n",
        "        if value == item_to_find:\n",
        "            indices.append(idx)\n",
        "    return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsuGnvZMNYX2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "labels_tokenized = []\n",
        "idx =-1\n",
        "for hct, et in zip(Tws_tokenized, Ent_tokenized):\n",
        "    idx+=1\n",
        "    labels = []\n",
        "    for i in range(len(hct)):\n",
        "        #Labels: 0->'O'; 1->'B'; 2->'I'\n",
        "        #labels.append('O')\n",
        "        labels.append(0)\n",
        "\n",
        "    #For Entities (Diseases|Enfermedades)\n",
        "    for enf in et:\n",
        "      first = True\n",
        "      for e in enf:\n",
        "          if first == True:\n",
        "              try:\n",
        "                #labels[hct.index(e)] = 'B'\n",
        "                #labels[posLab] = 'B'\n",
        "                indices = find_idx(hct, e)\n",
        "                if len(indices) > 1:\n",
        "                  for id in indices:\n",
        "                      labels[id] = 1\n",
        "                else:\n",
        "                  labels[hct.index(e)] = 1\n",
        "\n",
        "                first = False\n",
        "              except:\n",
        "                first = False\n",
        "                continue\n",
        "          else:\n",
        "              try:\n",
        "                #labels[hct.index(e)] = 'I'\n",
        "                #labels[posLab] = 'I'\n",
        "                indices = find_idx(hct, e)\n",
        "                if len(indices) > 1:\n",
        "                  for id in indices:\n",
        "                      if labels[id-1] != 0:\n",
        "                        labels[id] = 2\n",
        "                else:\n",
        "                  labels[hct.index(e)] = 2\n",
        "              except:\n",
        "                continue\n",
        "\n",
        "    labels_tokenized.append(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Tws[Tws_ids_training[48]])\n",
        "print(Tws_ids_training[48])"
      ],
      "metadata": {
        "id": "IlbbJdKv9UUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo_WB3uUWmlK"
      },
      "outputs": [],
      "source": [
        "j = 48\n",
        "for i in range(len(Tws_tokenized[j])):\n",
        "  print(str(Tws_tokenized[j][i]) + \"\\t\" + str(labels_tokenized[j][i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0OdnKiDSCn-"
      },
      "source": [
        "## Validating tokenization and alignment with the BIO tags.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61hktW4PR-bt"
      },
      "outputs": [],
      "source": [
        "flag = 0\n",
        "for st, lt in zip(Tws_tokenized, labels_tokenized):\n",
        "    if len(st) != len(lt):\n",
        "        print(st)\n",
        "        print(lt)\n",
        "        flag = 1\n",
        "if flag==0:\n",
        "    print(\"Everything is aligned!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence tokenization is an alternative but we finally used the whole tweet as samples."
      ],
      "metadata": {
        "id": "fPDzmJE6Wgho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeFfLm3Ubu7x"
      },
      "outputs": [],
      "source": [
        "sent_tokenized = []\n",
        "label_sent_tokenized = []\n",
        "for ht, lht in zip(Tws_tokenized, labels_tokenized):\n",
        "  st = []; lbst = []\n",
        "  for h, l in zip(ht,lht):\n",
        "    if h != \".\":\n",
        "      st.append(h)\n",
        "      lbst.append(l)\n",
        "    else:\n",
        "      st.append(\".\")\n",
        "      lbst.append(0)\n",
        "      sent_tokenized.append(st)\n",
        "      label_sent_tokenized.append(lbst)\n",
        "      st = []; lbst = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT7Ig0bcmOQR"
      },
      "outputs": [],
      "source": [
        "len(sent_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czWqzs_pmch5"
      },
      "outputs": [],
      "source": [
        "sent_tokenized[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOsSFm0wmS2I"
      },
      "outputs": [],
      "source": [
        "len(label_sent_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx6NtMpxmg2N"
      },
      "outputs": [],
      "source": [
        "label_sent_tokenized[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RIR3iRCUc0h"
      },
      "source": [
        "# Disease mentions identification as a Token classification problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WaBFjAEUc0l"
      },
      "source": [
        "## Install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEX7xfLiUc0m"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the followin line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9FTB991Sl12"
      },
      "source": [
        "## Building the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KVP5DQ7ZLuG"
      },
      "outputs": [],
      "source": [
        "dic = {\"tokens\": Tws_tokenized, \"ner_tags\": labels_tokenized} #For the whole clinical case. We used this option for our paper.\n",
        "#dic = {\"tokens\": sent_tokenized, \"ner_tags\": label_sent_tokenized} #Use this option if you want to check the model performance with sentences tokenized by \". \" but the whole clinical cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZawOY4qqSoz6"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "dataset = Dataset.from_dict(dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTBNV4yzYuzc"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMZbF0mSaOiS"
      },
      "outputs": [],
      "source": [
        "#For training, validation, and test partitions\n",
        "\"\"\"\n",
        "#Train, val, test partitions\n",
        "train_test = dataset.train_test_split()\n",
        "test_val = train_test['test'].train_test_split()\n",
        "raw_datasets = DatasetDict({\n",
        "    'train': train_test['train'],\n",
        "    'validation': test_val['train'],\n",
        "    'test': test_val['test']\n",
        "    })\n",
        "\"\"\"\n",
        "\n",
        "#Just for training and validation partitions\n",
        "train_test = dataset.train_test_split()\n",
        "raw_datasets = DatasetDict({\n",
        "    'train': train_test['train'],\n",
        "    'validation': train_test['test']\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFQfd5u9Uc0q"
      },
      "outputs": [],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI0wWv0aUc0t"
      },
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"][0][\"ner_tags\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-usgjZalK7E9"
      },
      "outputs": [],
      "source": [
        "raw_datasets['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLHqEaOVUc0u"
      },
      "outputs": [],
      "source": [
        "label_names = ['O','B','I']    #BIO scheme\n",
        "#label_names = ['O','I']    #IO scheme\n",
        "label_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iCYOiwZUc0u"
      },
      "outputs": [],
      "source": [
        "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
        "labels = [int(n) for n in raw_datasets[\"train\"][0][\"ner_tags\"]]\n",
        "#labels = raw_datasets[\"train\"][0][\"pos_tags\"]\n",
        "#labels = raw_datasets[\"train\"][0][\"chunk_tags\"]\n",
        "line1 = \"\"\n",
        "line2 = \"\"\n",
        "for word, label in zip(words, labels):\n",
        "    full_label = label_names[label]\n",
        "    max_length = max(len(word), len(full_label))\n",
        "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
        "\n",
        "print(line1)\n",
        "print(line2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading mBERT as a pre-trained model"
      ],
      "metadata": {
        "id": "CRKUqAVVbvf7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuZTRxLPUc0v"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"ajtamayoh/NER_EHR_Spanish_model_Mulitlingual_BERT\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr7qjUe7Uc0v"
      },
      "outputs": [],
      "source": [
        "tokenizer.is_fast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbchCgPGUc0v"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
        "inputs.tokens()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udiVaGenUc0w"
      },
      "outputs": [],
      "source": [
        "inputs.word_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhu4Cl3-Uc0w"
      },
      "outputs": [],
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            # Start of a new word!\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            # Special token\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            # Same word as previous token\n",
        "            label = labels[word_id]\n",
        "            # If the label is B-XXX we change it to I-XXX\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "\n",
        "    return new_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H820wg0sUc0x"
      },
      "outputs": [],
      "source": [
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "word_ids = inputs.word_ids()\n",
        "print(labels)\n",
        "print(align_labels_with_tokens(labels, word_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-ZxnhGSUc0x"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
        "    )\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INNjCms4Uc0y"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFpUXjvmUc0y"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHCuXD5EUc0y"
      },
      "outputs": [],
      "source": [
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
        "batch[\"labels\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6GW37IPUc0z"
      },
      "outputs": [],
      "source": [
        "for i in range(2):\n",
        "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImHtQb6HUc0z"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDjn2La4Uc0z"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mqjInn4Uc0z"
      },
      "outputs": [],
      "source": [
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "labels = [label_names[i] for i in labels]\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIaLt4n-Uc00"
      },
      "outputs": [],
      "source": [
        "predictions = labels.copy()\n",
        "predictions[2] = \"I\"\n",
        "metric.compute(predictions=[predictions], references=[labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xewJhf6Uc00"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdmwR_2HUc00"
      },
      "outputs": [],
      "source": [
        "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svP0J1bWs3pK"
      },
      "outputs": [],
      "source": [
        "id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvIvq5wxs8P8"
      },
      "outputs": [],
      "source": [
        "label2id"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing the head of prediction for Disease Mentions Identification under the BIO scheme"
      ],
      "metadata": {
        "id": "FKNvor68cVQs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9Rrve75Uc00"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    num_labels = 3,   #for BIO scheme\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDiAYqsYUc00"
      },
      "outputs": [],
      "source": [
        "model.config.num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS-L0U1XUc01"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"NLP-CIC-WFU_SocialDisNER_fine_tuned_NER_EHR_Spanish_model_Mulitlingual_BERT_v2\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=7,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning mBERT for Disease mentions identification"
      ],
      "metadata": {
        "id": "tdHv4CMvctoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face Authentication\n",
        "\n",
        "If you want to save your own model and make it available online we strongly recommend signing up at: https://huggingface.co/"
      ],
      "metadata": {
        "id": "K90ROU-SaLC5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgxCB7y8Uc0n"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hstit1gRUc0o"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"ajtamayoh@gmail.com\"\n",
        "!git config --global user.name \"ajtamayoh\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXbs9i1fUc0o"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6SkGUEUUc0p"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvHls_JJUc01"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the fine-tuned model at Hugging Face (It requires previous authentication)"
      ],
      "metadata": {
        "id": "0rGdJ42qc3sh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAvTQZClUc01"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(commit_message=\"Training complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "485I5jdmd6Dr"
      },
      "source": [
        "## Analyzing predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk5cSus7d5cA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1puArlhRe7_o"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "print(raw_datasets[\"validation\"][i]['tokens'])\n",
        "for j in range(len(preds[i])):\n",
        "  print(raw_datasets[\"validation\"][i]['ner_tags'][j], \"\\t\", preds[i][j])\n",
        "print(' '.join(raw_datasets[\"validation\"][i]['tokens']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjEM4d77Toxs"
      },
      "source": [
        "## Loading the model for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKO5fyKyUc04"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#Replace this with your own checkpoint. If you have run all the previous cells successfully, the model should be available at your hugging face account with the name: NLP-CIC-WFU_SocialDisNER_fine_tuned_NER_EHR_Spanish_model_Mulitlingual_BERT_v2\n",
        "model_checkpoint = \"ajtamayoh/NLP-CIC-WFU_SocialDisNER_fine_tuned_NER_EHR_Spanish_model_Mulitlingual_BERT_v2\"\n",
        "\n",
        "token_classifier = pipeline(\n",
        "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr7QlXqoaaMZ"
      },
      "outputs": [],
      "source": [
        "#pred = token_classifier(\"Â¿QuÃ© probabilidad hay de no pillar la gripe si me paso la vida en el hospital?\")\n",
        "pred = token_classifier(\"Wanna collaborate in breast cancer research? Quieres colaborar en nuestra lucha contra el cÃ¡ncer de mama? https://t.co/P53hAhcdb5 #cancer #breastcancer #cancerdemama https://t.co/wP9pVG41UW\")\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL4mdo1umfTJ"
      },
      "outputs": [],
      "source": [
        "val_path = \"/content/drive/MyDrive/Dataset/train-valid-txt-files/validation/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "val_file_names = listdir(val_path)"
      ],
      "metadata": {
        "id": "h23Duy2Uruh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF2yE2iJtAJy"
      },
      "outputs": [],
      "source": [
        "i = 14\n",
        "with open(val_path + val_file_names[i], \"r\", encoding=\"UTF-8\") as ftest:\n",
        "  pred = token_classifier(ftest.read())\n",
        "pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Processing"
      ],
      "metadata": {
        "id": "5SXdOl-E6Qk6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blQLws4pLuBP"
      },
      "outputs": [],
      "source": [
        "def grouping_entities(pred):\n",
        "  import re\n",
        "  output = []\n",
        "  for e in pred:\n",
        "    if \"##\" not in e['word']:\n",
        "      output.append(e)\n",
        "    else:\n",
        "      try:\n",
        "        if e['start'] == (output[-1]['end']):\n",
        "          output[-1]['word'] = output[-1]['word']+re.sub(\"##\",\"\",e['word'])\n",
        "          output[-1]['end'] = e['end']\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "      if (e['entity_group'] == \"B\" or e['entity_group'] == \"I\") and (e['start'] == (output[-2]['end']+1)):\n",
        "        output[-2]['word'] = output[-2]['word']+\" \"+e['word']\n",
        "        output[-2]['end'] = e['end']\n",
        "        output.pop(-1)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      if e['start'] == (output[-2]['end']):\n",
        "        output[-2]['word'] = output[-2]['word']+e['word']\n",
        "        output[-2]['end'] = e['end']\n",
        "        output.pop(-1)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hPi30NgqtDQ"
      },
      "outputs": [],
      "source": [
        "grouping_entities(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions on validation dataset"
      ],
      "metadata": {
        "id": "WndToMHQZ0GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_accents(s):\n",
        "  l = [('Ã¡', 'a'), ('Ã©','e'), ('Ã­','i'), ('Ã³','o'), ('Ãº','u')]\n",
        "  for v in l:\n",
        "    s = s.replace(v[0],v[1])\n",
        "  return s"
      ],
      "metadata": {
        "id": "hdWNhwFvK5RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing...\")\n",
        "import re\n",
        "f = open(\"/content/drive/MyDrive/dev_predictions_SocialDisNER_model_v2.tsv\", \"w\", encoding=\"UTF-8\")\n",
        "f.write(\"tweets_id\\tbegin\\tend\\ttype\\textraction\\n\")\n",
        "for fname in val_file_names:\n",
        "  print(f\"Text: {fname}\", end=\"\\r\")\n",
        "  with open(val_path + fname, \"r\", encoding=\"UTF-8\") as fval:\n",
        "    lista_spans = []\n",
        "    offs = []\n",
        "    hc = fval.read()\n",
        "    pred = token_classifier(hc)\n",
        "    pred_grouped = grouping_entities(pred)\n",
        "    t = 1\n",
        "    for p in pred_grouped:\n",
        "\n",
        "      off0 = int(p['start'])\n",
        "      off1 = int(p['end'])\n",
        "      span = p['word']\n",
        "\n",
        "      span = re.sub(\"^, |^,|^\\. |^\\.|^: |^:|^; |^;|^\\( |^\\(|^\\) |^\\)\",\"\",span)\n",
        "\n",
        "      if \"\\n\" in span:\n",
        "        span = re.sub(\"\\n\",\" \",span)\n",
        "\n",
        "      if \" - \" in span:\n",
        "        span = re.sub(\" - \",\"-\",span)\n",
        "\n",
        "      if \"- \" in span:\n",
        "        span = re.sub(\"- \",\"-\",span)\n",
        "\n",
        "      if \" -\" in span:\n",
        "        span = re.sub(\" -\",\"-\",span)\n",
        "\n",
        "      if \"( \" in span:\n",
        "        span = re.sub(\"\\( \",\"(\",span)\n",
        "\n",
        "      if \" )\" in span:\n",
        "        span = re.sub(\" \\)\",\")\",span)\n",
        "\n",
        "      if span.endswith(\" y\") :\n",
        "        span = span[:-2]\n",
        "\n",
        "      if span.endswith(\" de\") or span.endswith(\" en\"):\n",
        "        span = span[:-3]\n",
        "\n",
        "      if span.endswith(\" por\") or span.endswith(\" con\") or span.endswith(\" del\"):\n",
        "        span = span[:-4]\n",
        "\n",
        "      if span.endswith(\".\") or span.endswith(\",\") or span.endswith(\";\") or span.endswith(\":\") or span.endswith(\"â€“\") or span.endswith(\"-\") or span.endswith(\"!\"):\n",
        "        span = span[:-1]\n",
        "\n",
        "      if span.endswith(\" .\") or span.endswith(\" ,\") or span.endswith(\" ;\") or span.endswith(\" :\") or span.endswith(\" â€“\") or span.endswith(\" -\"):\n",
        "        span = span[:-2]\n",
        "\n",
        "      if span.startswith(\"#\"):\n",
        "        span = span[1:]\n",
        "\n",
        "      if span.startswith(\"ðŸ”¸ \"):\n",
        "        span = span[2:]\n",
        "\n",
        "      pattern = r\"^[a-z|Ã¡|Ã©|Ã­|Ã³|Ãº|/]{0,2}$|^[0-9]+$|^[A-Z]$|^#$| a$| el$| la$|^ðŸ”¸$\"\n",
        "      match = re.findall(pattern, span)\n",
        "      if len(match) > 0:\n",
        "        continue\n",
        "\n",
        "      if span not in lista_spans:\n",
        "        #For multiword spans\n",
        "        mwspan = \"\".join(span.split())\n",
        "        if span != mwspan:\n",
        "          spans = [span, mwspan]\n",
        "        else:\n",
        "          spans = [span]\n",
        "        # Find all indices of 'span'\n",
        "        for sp in spans:\n",
        "          indices = [index for index in range(len(hc)) if delete_accents(hc.lower()).startswith(delete_accents(sp.lower()), index)]\n",
        "          for ind in indices:\n",
        "            off0 = ind\n",
        "            off1 = ind+len(sp)\n",
        "            extraction = hc[off0:off1]\n",
        "            match = re.findall(pattern, extraction)\n",
        "            no_subsumed = True\n",
        "            for of in offs:\n",
        "              if off0 == of[0] and off1 < of[1]:\n",
        "                no_subsumed = False\n",
        "            output = fname[:-4]+\"\\t\"+str(off0)+\"\\t\"+str(off1)+\"\\t\"+\"ENFERMEDAD\"+\"\\t\"+extraction+\"\\n\"\n",
        "            if len(match) == 0 and no_subsumed:\n",
        "              f.write(output)\n",
        "              t+=1\n",
        "              offs.append((off0,off1))\n",
        "\n",
        "          lista_spans.append(sp)\n",
        "\n",
        "f.close()\n",
        "print(\"Completo.\")"
      ],
      "metadata": {
        "id": "1lfSbDwNaBHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions on test dataset"
      ],
      "metadata": {
        "id": "zevVCf3-g2AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = \"/content/drive/MyDrive/Dataset/test-data/test-data-txt-files/\""
      ],
      "metadata": {
        "id": "Z9U8F4ywhxlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "test_file_names = listdir(test_path)"
      ],
      "metadata": {
        "id": "g0NS3wuMhvxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_file_names)"
      ],
      "metadata": {
        "id": "q3hLnqMZjTwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing...\")\n",
        "import re\n",
        "f = open(\"/content/drive/MyDrive/test_predictions.tsv\", \"a\", encoding=\"UTF-8\")\n",
        "f.write(\"tweets_id\\tbegin\\tend\\ttype\\textraction\\n\")\n",
        "for fname in test_file_names:\n",
        "  print(f\"Text: {fname}\", end=\"\\r\")\n",
        "  with open(test_path + fname, \"r\", encoding=\"UTF-8\") as fval:\n",
        "    lista_spans = []\n",
        "    offs = []\n",
        "    hc = fval.read()\n",
        "    pred = token_classifier(hc)\n",
        "    pred_grouped = grouping_entities(pred)\n",
        "    t = 1\n",
        "    for p in pred_grouped:\n",
        "\n",
        "      off0 = int(p['start'])\n",
        "      off1 = int(p['end'])\n",
        "      span = p['word']\n",
        "\n",
        "      span = re.sub(\"^, |^,|^\\. |^\\.|^: |^:|^; |^;|^\\( |^\\(|^\\) |^\\)\",\"\",span)\n",
        "\n",
        "      if \"\\n\" in span:\n",
        "        span = re.sub(\"\\n\",\" \",span)\n",
        "\n",
        "      if \" - \" in span:\n",
        "        span = re.sub(\" - \",\"-\",span)\n",
        "\n",
        "      if \"- \" in span:\n",
        "        span = re.sub(\"- \",\"-\",span)\n",
        "\n",
        "      if \" -\" in span:\n",
        "        span = re.sub(\" -\",\"-\",span)\n",
        "\n",
        "      if \"( \" in span:\n",
        "        span = re.sub(\"\\( \",\"(\",span)\n",
        "\n",
        "      if \" )\" in span:\n",
        "        span = re.sub(\" \\)\",\")\",span)\n",
        "\n",
        "      if span.endswith(\" y\") :\n",
        "        span = span[:-2]\n",
        "\n",
        "      if span.endswith(\" de\") or span.endswith(\" en\"):\n",
        "        span = span[:-3]\n",
        "\n",
        "      if span.endswith(\" por\") or span.endswith(\" con\") or span.endswith(\" del\"):\n",
        "        span = span[:-4]\n",
        "\n",
        "      if span.endswith(\".\") or span.endswith(\",\") or span.endswith(\";\") or span.endswith(\":\") or span.endswith(\"â€“\") or span.endswith(\"-\") or span.endswith(\"!\"):\n",
        "        span = span[:-1]\n",
        "\n",
        "      if span.endswith(\" .\") or span.endswith(\" ,\") or span.endswith(\" ;\") or span.endswith(\" :\") or span.endswith(\" â€“\") or span.endswith(\" -\"):\n",
        "        span = span[:-2]\n",
        "\n",
        "      if span.startswith(\"#\"):\n",
        "        span = span[1:]\n",
        "\n",
        "      if span.startswith(\"ðŸ”¸ \"):\n",
        "        span = span[2:]\n",
        "\n",
        "      pattern = r\"^[a-z|Ã¡|Ã©|Ã­|Ã³|Ãº|/]{0,2}$|^[0-9]+$|^[A-Z]$|^#$| a$| el$| la$|^ðŸ”¸$\"\n",
        "      match = re.findall(pattern, span)\n",
        "      if len(match) > 0:\n",
        "        continue\n",
        "\n",
        "      if span not in lista_spans:\n",
        "        #For multiword spans\n",
        "        mwspan = \"\".join(span.split())\n",
        "        if span != mwspan:\n",
        "          spans = [span, mwspan]\n",
        "        else:\n",
        "          spans = [span]\n",
        "        # Find all indices of 'span'\n",
        "        for sp in spans:\n",
        "          indices = [index for index in range(len(hc)) if delete_accents(hc.lower()).startswith(delete_accents(sp.lower()), index)]\n",
        "          for ind in indices:\n",
        "            off0 = ind\n",
        "            off1 = ind+len(sp)\n",
        "            extraction = hc[off0:off1]\n",
        "            match = re.findall(pattern, extraction)\n",
        "            no_subsumed = True\n",
        "            for of in offs:\n",
        "              if off0 == of[0] and off1 < of[1]:\n",
        "                no_subsumed = False\n",
        "            output = fname[:-4]+\"\\t\"+str(off0)+\"\\t\"+str(off1)+\"\\t\"+\"ENFERMEDAD\"+\"\\t\"+extraction+\"\\n\"\n",
        "            if len(match) == 0 and no_subsumed:\n",
        "              f.write(output)\n",
        "              t+=1\n",
        "              offs.append((off0,off1))\n",
        "\n",
        "          lista_spans.append(sp)\n",
        "\n",
        "f.close()\n",
        "print(\"Completo.\")"
      ],
      "metadata": {
        "id": "6pM_3vPeg3uA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}